import streamlit as st
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from streamlit_ace import st_ace
import matplotlib.pyplot as plt
import io, sys, textwrap

def show():
    # 1) ìˆ˜ì—´ ì˜ˆì¸¡ ì‚¬ì „ ì§€ì‹ í•™ìŠµì§€ (Markdown ê·¸ëŒ€ë¡œ)
    pre_knowledge = r'''
    ##### 1. **ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ìˆ˜ì—´ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆì„ê¹Œ?**

    - **ìˆ˜ì—´ ë¬¸ì œ**: 2, 5, 8, 11 â€¦ ì²˜ëŸ¼ ê·œì¹™ì„ ì°¾ì•„ ë‹¤ìŒ ìˆ«ìë¥¼ ë§ì¶”ëŠ” ê²Œì„ê³¼ ë¹„ìŠ·í•¨
    - **ë¨¸ì‹ ëŸ¬ë‹ì˜ ì¥ì **: ì‚¬ëŒì´ ê·œì¹™ì„ ì¼ì¼ì´ ìˆ˜ì‹ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šì•„ë„, ì˜ˆì‹œ(ë°ì´í„°)ë¥¼ ë³´ì—¬ì£¼ë©´ ê¸°ê³„ê°€ ìŠ¤ìŠ¤ë¡œ íŒ¨í„´ì„ ì°¾ìŒ
    - **ì‘ìš© ì˜ˆì‹œ**:
        - ì‹œí—˜ ì ìˆ˜ ì¶”ì„¸ë¡œ ë‹¤ìŒ ì‹œí—˜ ì˜ˆìƒ
        - ê¸°ì˜¨ ë³€í™”ë¡œ ë‚´ì¼ ë‚ ì”¨ ì˜ˆì¸¡

    ##### 2. **ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë„êµ¬**
    - **ë°ì´í„°(ìˆ˜ì—´)**: í•™ìŠµí•  ìˆ«ìì˜ ë‚˜ì—´(ìˆ˜ì—´)
    - **ëª¨ë¸(ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜)**:
        - ì„ í˜• íšŒê·€(Linear Regression): ì§ì„  ì¶”ì„¸ì„ 
        - ë‹¤í•­ íšŒê·€(Polynomial Regression) ê³¡ì„  ì¶”ì„¸ì„ 
    - **ì½”ë”© ë„êµ¬**:
        - Python
        - NumPy (ìˆ«ì ê³„ì‚°)
        - scikit-learn (ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬)

    ##### 3. **í•µì‹¬ ê°œë…: ì–´ë–»ê²Œ ë™ì‘í• ê¹Œ?**
    - **ìˆ«ìì— â€˜ìœ„ì¹˜â€™ë¥¼ ë¶™ì—¬ìš”**
        - ìˆ˜ì—´ì´ `[2, 5, 8, 11]`ì¼ ë•Œ
        - ìœ„ì¹˜ 1 â†’ ê°’ 2
        - ìœ„ì¹˜ 2 â†’ ê°’ 5
        - ìœ„ì¹˜ 3 â†’ ê°’ 8
    - **ëª¨ë¸ì—ê²Œ ì•Œë ¤ì¤˜ìš”(í•™ìŠµ)**
        - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ìœ„ì¹˜(ì…ë ¥ê°’)ì™€ ê°’(ì¶œë ¥ê°’)ì„ í•¨ê»˜ ë³´ì—¬ì£¼ë©´, íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
    - **ë‹¤ìŒ ìœ„ì¹˜ ì˜ˆì¸¡í•˜ê¸°**
        - ì˜ˆ: 5ë²ˆì§¸ ìˆ«ìë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´ `5`ë¥¼ ëª¨ë¸ì— ë„£ìœ¼ë©´, ì˜ˆì¸¡ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ##### 3. íšŒê·€ ë¶„ì„(Regression) ê°œìš”

    - **íšŒê·€ ë¶„ì„ì´ë€?**
      - **ìˆ˜(number)** ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í†µê³„ì Â·ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•.

    - **ì„ í˜• íšŒê·€(Linear Regression)**
      - ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì§ì„  ë°©ì •ì‹ì„ ì°¾ìŒ.
      - ì˜ˆì¸¡ê°’: $\hat y = w_0 + w_1 x$

      - $w_1$: ê¸°ìš¸ê¸°(slope), $w_0$: ì ˆí¸(intercept)
      - **ì†ì‹¤ í•¨ìˆ˜(Loss)**: $\displaystyle \mathrm{MSE} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat y_i)^2$

    - **ë‹¤í•­ íšŒê·€(Polynomial Regression)**
      - ì…ë ¥ $x$ë¥¼ $[x, x^2, â€¦, x^d]$ë¡œ í™•ì¥ í›„ **ì„ í˜• íšŒê·€** ì ìš©
      - ì˜ˆì¸¡ê°’:
    $$
    \hat y â€‹= w_0â€‹+w_1â€‹x+w_2â€‹x^2+â‹¯+w_nâ€‹x^n
    $$

    ##### 4. **ì‹¤ìŠµ íë¦„**

    1. ìˆ˜ì—´ ì…ë ¥: `2, 5, 8, 11`
    2. ëª¨ë¸ ì„ íƒ: ì„ í˜• / ë‹¤í•­
    3. í•™ìŠµ ì‹¤í–‰: `model.fit(X, y)`
    4. ì˜ˆì¸¡ ì‹¤í–‰: `model.predict([[5]])`
    5. ê²°ê³¼ í™•ì¸: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ

    > **Tip:** ì•±ì˜ ìŠ¬ë¼ì´ë”ì™€ ë¼ë””ì˜¤ ë²„íŠ¼ìœ¼ë¡œ ì…ë ¥ì„ ë°”ê¿” ê°€ë©°, ì˜ˆì¸¡ ê²°ê³¼ ë³€í™”ë¥¼ ì‚´í´ë³´ì„¸ìš”!

    ##### 5. **ë§ˆë¬´ë¦¬**

    - ë¨¸ì‹ ëŸ¬ë‹ì€ **ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë„êµ¬**ì…ë‹ˆë‹¤.
    - ê°„ë‹¨í•œ ìˆ˜ì—´ ì˜ˆì¸¡ë¶€í„° ì‹œì‘í•´, ì ì°¨ ë³µì¡í•œ ë°ì´í„°(ë‚ ì”¨, ì£¼ê°€, ê·¸ë¦¼Â·ì†Œë¦¬ ë“±)ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - ì˜¤ëŠ˜ ë°°ìš´ **â€˜ìœ„ì¹˜ â†’ ê°’â€™** ê°œë…ê³¼ **ì„ í˜•Â·ë‹¤í•­ íšŒê·€**ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§ì ‘ ì½”ë“œë¥¼ ìˆ˜ì •Â·í™•ì¥í•´ ë³´ì„¸ìš”!
    '''   

    # 2) ìˆ˜ì—´ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ êµìœ¡ ìë£Œ (expander)
    ml_material = '''
    ##### ğŸ“–**í•™ìŠµ ëª©í‘œ**

    - ì£¼ì–´ì§„ íŒŒì´ì¬ ì½”ë“œë¥¼ ì½ê³ , ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ìˆ˜ì—´ì„ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì„ ì´í•´í•œë‹¤.
    - `NumPy`, `scikit-learn`ì˜ ì£¼ìš” í•¨ìˆ˜ì™€ ë©”ì„œë“œë¥¼ í™œìš©í•´ ì…ì¶œë ¥ ë°ì´í„° í˜•íƒœë¥¼ ë³€í™˜í•˜ëŠ” ë²•ì„ ìµíŒë‹¤.
    - ì„ í˜• íšŒê·€ì™€ ë‹¤í•­ íšŒê·€ì—ì„œ ê°ê° ì–´ë–¤ ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ê°œë…ì ìœ¼ë¡œ íŒŒì•…í•œë‹¤.

    ---
    ##### ğŸ“–**ì½”ë“œ ì„¤ëª…**
    ğŸ”– **ì„ í˜• íšŒê·€ ëª¨ë¸**

    ```python
     1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
        import numpy as np
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import PolynomialFeatures
    ``` 
    - **`numpy`**: ë°°ì—´ ìƒì„±Â·ì—°ì‚°ì„ ì‰½ê²Œ ë„ì™€ì£¼ëŠ” íŒ¨í‚¤ì§€
    - **`LinearRegression`**: ì§ì„  í˜•íƒœ ëª¨ë¸ë¡œ í•™ìŠµÂ·ì˜ˆì¸¡
    - **`PolynomialFeatures`**: ì…ë ¥ê°’ì„ ë‹¤í•­ì‹ í•­ $(x, x^2, x^3,...)$ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë„êµ¬

    ```python 
    2) í•™ìŠµìš© ìˆ˜ì—´ê³¼ ì˜ˆì¸¡í•  í•­ ë²ˆí˜¸
    seq = [2, 5, 8, 11]      # í•™ìŠµí•  ìˆ˜ì—´ ë°ì´í„°
    n   = 5                  # ì˜ˆì¸¡í•  í•­ì˜ ë²ˆí˜¸ (5ë²ˆì§¸ í•­)
    ```
    - **`seq`**: ì˜ˆì‹œë¡œ ì‚¬ìš©í•  ìˆ˜ì—´, ì˜ˆ) `[2, 5, 8, 11]`
    - **`n`**: ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ "ë‹¤ìŒ" í˜¹ì€ "íŠ¹ì •" í•­ì˜ ë²ˆí˜¸, ì˜ˆ) 5
    ```python
    3) ì…ë ¥ Xì™€ ì¶œë ¥ y ì¤€ë¹„
    X   = np.arange(1, len(seq)+1).reshape(-1, 1)
    # ìœ„ì¹˜ ì •ë³´ë¥¼ 2ì°¨ì› ë°°ì—´(ì—´ ë²¡í„°)ë¡œ ë³€í™˜
    y   = np.array(seq)       # ìˆ˜ì—´ ê°’ì„ 1ì°¨ì› ë°°ì—´ë¡œ ì¤€ë¹„
    ```
    - `np.arange(1, len(seq)+1)` â†’ `[1, 2, 3, 4]`: ìˆ˜ì—´ ìœ„ì¹˜ ìƒì„±
    - `.reshape(-1, 1)` â†’ `[[1], [2], [3], [4]]`: 2ì°¨ì› ì—´ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜
    - `[2, 5, 8, 11]` â†’ 1ì°¨ì› ë°°ì—´
    - **1ì°¨ì›**ì€ â€˜ë‹¨ì¼ ì¶œë ¥â€™ì„ ìœ„í•œ ìì—°ìŠ¤ëŸ¬ìš´ í˜•íƒœ
    ```python
     4) ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
    model = LinearRegression()
    model.fit(X, y)
    ```
    - `fit(X, y)` â†’ `X`ì™€ `y`ë¥¼ ë³´ê³  **ìµœì ì˜ ê¸°ìš¸ê¸°ì™€ ì ˆí¸**ì„ ê³„ì‚°
    - í•™ìŠµëœ ì§ì„ ì€  $\hat y= w_1 x + w_0 $ í˜•íƒœ
    ```python
    5) ì˜ˆì¸¡ ë° ê²°ê³¼ ì¶œë ¥
    pred = model.predict([[n]])[0]
    print(pred)  # në²ˆì§¸ ì˜ˆì¸¡ê°’
    ```
    - `[[n]]` â†’ shape `(1,1)`ì¸ 2ì°¨ì› ì…ë ¥
    - `predict` ê²°ê³¼ëŠ” 1ì°¨ì› ë°°ì—´(`array([ê°’])`) â†’ `[0]`ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ ê°’ì„ ì¶”ì¶œ
    - ì˜ˆì¸¡ê°’ì„ í™”ë©´ì— ì¶œë ¥

    ---

    **ğŸ”–ë‹¤í•­ íšŒê·€ ëª¨ë¸ í™•ì¥**
    ```python
    1) ë‹¤í•­ ë³€í™˜ê¸° ì‚¬ìš©
    poly  = PolynomialFeatures(degree=3, include_bias=False)
    ```
    - `degree=3` â†’ $[x, x^2, x^3]$ íŠ¹ì„± ìƒì„±
    - `include_bias=False` â†’ ìƒìˆ˜í•­ $x^0=1$ ì œì™¸
    ```python
    2) ë³€í™˜ í›„ í•™ìŠµ
    Xp = poly.fit_transform(X)
    model = LinearRegression()
    model.fit(Xp, y)
    ```
    - `Xp` ëŠ” `X`ë¥¼ $d$ ì°¨í•­ê¹Œì§€ í™•ì¥í•œ 2ì°¨ì› ë°°ì—´
    - ë™ì¼í•œ `LinearRegression`ìœ¼ë¡œ í•™ìŠµí•˜ë˜, ì…ë ¥ íŠ¹ì„±ì´ ëŠ˜ì–´ë‚¨

    ```python
    3) ë‹¤í•­ ì˜ˆì¸¡
    pred = model.predict(poly.transform([[n]]))[0]
    print(pred)
    ```
    - `poly.transform([[n]])` â†’ $[n, n^2, n^3]$ í˜•íƒœë¡œ ë³€í™˜
    '''

    with st.expander("ğŸ“š **ë¨¸ì‹ ëŸ¬ë‹ì˜ ì´í•´**"):
        st.markdown(pre_knowledge)
    with st.expander("ğŸ“ **ë¨¸ì‹ ëŸ¬ë‹ ìˆ˜ì—´ ì˜ˆì¸¡ íŒŒì´ì¬ ì½”ë“œì˜ ì´í•´**"):
        st.markdown(ml_material)
        
    st.divider() 
    st.markdown("### ğŸš€ìˆ˜ì—´ ì˜ˆì¸¡")

    # 1) ëª¨ë¸ ì„ íƒ
    model = st.radio("ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”", ["LinearRegression", "PolynomialRegression"])
    degree = st.slider("ë‹¤í•­ íšŒê·€ ì°¨ìˆ˜ ì„ íƒ", 2, 5, 2) if model=="PolynomialRegression" else None

    # 2) ìˆ˜ì—´ ì…ë ¥
    seq_input = st.text_input("ìˆ˜ì—´ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 2,5,8,11)", "2,5,8,11")

    # 3) ì˜ˆì¸¡í•  í•­ ë²ˆí˜¸ ì…ë ¥
    term_idx = st.number_input("ì˜ˆì¸¡í•  í•­ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                               min_value=1, value=len(seq_input.split(","))+1)

    # 4) ì½”ë“œ í…œí”Œë¦¿ ìƒì„±
    if model=="LinearRegression":
        raw = f"""
import numpy as np 
from sklearn.linear_model import LinearRegression

seq = [{seq_input}] #í•™ìŠµí•  ìˆ˜ì—´
n = {term_idx} #ì˜ˆì¸¡í•  í•­ì˜ ë²ˆí˜¸

X = np.arange(1, len(seq)+1).reshape(-1,1) #ì…ë ¥ê°’ì€ 2ì°¨ì› ë°°ì—´ ë³€í™”
y = np.array(seq) #ëª©í‘œê°’ì€ 1ì°¨ì› ë°°ì—´

model = LinearRegression() #ì„ í˜• íšŒê·€ëª¨ë¸
model.fit(X, y) #ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ

pred = model.predict([[n]])[0] #në²ˆì§¸ ì˜ˆì¸¡ê°’
print(pred)
"""
    else:
        raw = f"""
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

seq = [{seq_input}] #í•™ìŠµí•  ìˆ˜ì—´
n = {term_idx} #ì˜ˆì¸¡í•  í•­ì˜ ë²ˆí˜¸

X = np.arange(1, len(seq)+1).reshape(-1,1) #ì…ë ¥ê°’ì€ 2ì°¨ì› ë°°ì—´ ë³€í™”
y = np.array(seq) #ëª©í‘œê°’ì€ 1ì°¨ì› ë°°ì—´

poly = PolynomialFeatures(degree={degree}, include_bias=False) #nì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë³€í™˜ê¸°
Xp = poly.fit_transform(X) #ì…ë ¥ê°’ì„ ë³€í™˜
model = LinearRegression() #ì„ í˜• íšŒê·€ëª¨ë¸
model.fit(Xp, y) #ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ

pred = model.predict(poly.transform([[n]]))[0]
print(pred)
"""
    full_code = textwrap.dedent(raw)

    # 5) ACE ì—ë””í„°: í•­ìƒ ìµœì‹  full_code ë°˜ì˜
    signature = f"{model}|{seq_input}|{term_idx}|{degree}"
    st.markdown("#### ğŸ–¥ï¸ íŒŒì´ì¬ ì½”ë“œ ì—ë””í„° (ìˆ˜ì • ê°€ëŠ¥)")
    user_code = st_ace(
        value=full_code,
        language="python",
        theme="monokai",
        height=300,
        key=f"ace_{signature}"
    )

    # 6) ì‹¤í–‰ ë° ì‹œê°í™”
    if st.button("â–¶ï¸ ì˜ˆì¸¡ ì‹¤í–‰ ë° ì‹œê°í™”"):
        buf = io.StringIO()
        # execí•  ë•Œ ë¡œì»¬ ì‹¤í–‰ ê³µê°„ì„ dictë¡œ ë§Œë“¤ì–´ seq, pred, nì„ ë½‘ì•„ì˜µë‹ˆë‹¤.
        exec_locals = {}
        try:
            sys.stdout = buf
            exec(user_code, {}, exec_locals)
        finally:
            sys.stdout = sys.__stdout__

        # ìº¡ì²˜ëœ í”„ë¦°íŠ¸ ê²°ê³¼
        output = buf.getvalue().strip()
        st.success(f"ì‹¤í–‰ ê²°ê³¼: {output}")

        # ì‹œê°í™”
        seq = exec_locals.get("seq", [])
        pred = exec_locals.get("pred", None)
        n = exec_locals.get("n", None)

        if seq and (pred is not None) and (n is not None):
            fig, ax = plt.subplots()
            ax.plot(range(1, len(seq)+1), seq, marker="o", label="input sequence")
            ax.scatter(n, pred, color="red", label=f"{n}th predicted value")
            ax.set_title("Sequence and prediction results")
            ax.set_xlabel("number")
            ax.set_ylabel("value")
            ax.legend()
            ax.grid(True)
            st.pyplot(fig)
        else:
            st.warning("`seq`, `pred`, `n` The value is not properly defined.")

# ì‹¤ì œ ì•±ì—ì„œëŠ” ì•„ë˜ì²˜ëŸ¼ show()ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
if __name__ == "__main__":
    show()
